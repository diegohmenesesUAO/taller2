# -*- coding: utf-8 -*-
"""Taller 2 - modelo de regresión lineal estándar entrenado en el Boston house prices.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-Wk64PoBMfoDlMw_vJScGXa6iR88H9eu

*texto en cursiva*
# Taller 2 - Modelo de regresión lineal estándar entrenado en el Boston house prices

## Etapa 1: Entendimiento de los datos
"""

# Cargo librerías y hago lectura del archivo directo desde la URL, ya que el conjunto de datos sobre precios de la vivienda en Boston tiene un problema ético
import pandas as pd
import numpy as np

# URL de donde cargo los datos
data_url = "http://lib.stat.cmu.edu/datasets/boston"
# Cargo los datos crudos, es decir tal, como llegan
raw_df = pd.read_csv(data_url, sep="\s+", skiprows=22, header=None)
# Preprocesamiento para obtener las características y el objetivo
data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
target = raw_df.values[1::2, 2]

# Convertimos 'data' y 'target' en un DataFrame de pandas
columns = ["CRIM", "ZN", "INDUS", "CHAS", "NOX", "RM", "AGE", "DIS", "RAD", "TAX", "PTRATIO", "B", "LSTAT"]
data_df = pd.DataFrame(data, columns=columns)
data_df["MEDV"] = target  # Añadimos la columna objetivo con el nombre 'MEDV'

print(data_df.head())

#Cuál es el número de registros?
#Cuál es el número de atributos?

shape = data_df.shape
shape

#Cual es el tipo de los atributos?
data_df.dtypes

#Medida de centralidad y desviación para atributos numéricos:
data_df.describe()

# Diagrama de cajas y bigotes para atributos numéricos
# Permite identificar la existencia de datos atípicos
import matplotlib.pyplot as plt

# Preparamos los datos para el diagrama de cajas
data_to_plot = [data_df['CRIM'], data_df['ZN'], data_df['INDUS'], data_df['CHAS'], data_df['NOX'],
                data_df['RM'], data_df['AGE'], data_df['DIS'], data_df['RAD'], data_df['TAX'],
                data_df['PTRATIO'], data_df['B'], data_df['LSTAT'], data_df['MEDV']]

# Nombres de las columnas para usar como etiquetas
labels = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']

# Creación del diagrama de cajas
plt.figure(figsize=(12, 8))  # Ajustamos el tamaño para mejorar la visualización
plt.boxplot(data_to_plot, labels=labels)
plt.xticks(rotation=45)  # Rotamos las etiquetas del eje x para mejorar legibilidad
plt.title('Boxplot para los atributos del dataset de Boston')  # Título del gráfico
plt.ylabel('Valor')  # Etiqueta del eje y
plt.xlabel('Atributos')  # Etiqueta del eje x
plt.grid(True)  # Añadimos una rejilla para facilitar la lectura
plt.show()

#Medida de centralidad para atributos categóricos:
data_df.mode()

# Correlación entre los atributos de entrada numéricos
# Permite detectar si hay atributos redundantes (correlación mayor a 0.85 o menor a -0.85)
data_df.corr()

# Cual es el máximo de datos faltantes en un mismo registro?
# Si hay registros a los que les faltan muchos valores, es mejor eliminarlos.
max(data_df.isnull().sum(axis=1))

#Cuantos datos faltantes hay por cada atributo?
data_df.isnull().sum()

#Resumen que incluye varias estadísticas descriptivas importantes de la variable objetivo

print(data_df['MEDV'].describe())

"""## Etapa 2: Preparación de los datos





"""

from sklearn import preprocessing

# Se normalizan las características
features = data_df.columns[:-1]  # Todas las columnas excepto MEDV
data_df[features] = preprocessing.scale(data_df[features])

data_df.head()

"""## Etapa 3: Modelado

"""

# Divido el conjunto de datos en conjuntos de entrenamiento y test

from sklearn.model_selection import train_test_split

# Divido el conjunto de datos en características (X) y variable objetivo (y)
X = data_df.drop('MEDV', axis=1)  # Todas las columnas excepto 'MEDV'
y = data_df['MEDV']  # La columna 'MEDV'

# Divido los datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)

# Muestro las dimensiones de los conjuntos de entrenamiento y prueba para verificar
print("Dimensiones del conjunto de entrenamiento:", X_train.shape, y_train.shape)
print("Dimensiones del conjunto de prueba:", X_test.shape, y_test.shape)

# Entrenamiento del modelo
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Creo el modelo de regresión lineal
linear_model = LinearRegression()

# Entreno el modelo con los conjuntos de entrenamiento
linear_model.fit(X_train, y_train)

# Realizo predicciones sobre el conjunto de prueba
y_pred = linear_model.predict(X_test)

# Calculo el Error Cuadrático Medio (MSE) y el coeficiente de determinación (R^2)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Imprimo las métricas de evaluación
print("Error Cuadrático Medio (MSE):", mse)
print("Coeficiente de Determinación (R^2):", r2)

# Realizo predicciones sobre el conjunto de prueba con el modelo de regresión lineal
predictions = linear_model.predict(X_test)
# Imprimo las primeras 10 predicciones
print("Primeras 10 predicciones:", predictions[:10])

# evaluo el desempeño del modelo sobre los datos de test
from sklearn.metrics import mean_squared_error, r2_score

# Cálculo del Mean Squared Error (MSE)
mse = mean_squared_error(y_test, predictions)
print(f"Mean Squared Error (MSE): {mse}")

# Cálculo de la Raíz del Mean Squared Error (RMSE)
rmse = np.sqrt(mse)
print(f"Root Mean Squared Error (RMSE): {rmse}")

# Cálculo del Mean Absolute Error (MAE)
mae = np.mean(np.abs(predictions - y_test))
print(f"Mean Absolute Error (MAE): {mae}")

# Cálculo del Coeficiente de Determinación (R^2)
r2 = r2_score(y_test, predictions)
print(f"Coeficiente de Determinación (R^2): {r2}")

"""### **2. Con las características escaladas utilizando por lo menos tres métodos diferentes de escalado, los cuales puedes encontrar dentro del módulo preprocessing de sklearn.**"""

from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler

#Aplicando StandardScaler
#Aplicando MinMaxScaler
#Aplicando RobustScaler

# Escaladores
standard_scaler = StandardScaler()
minmax_scaler = MinMaxScaler()
robust_scaler = RobustScaler()

# Aplico escaladores a las características (excepto la variable objetivo 'MEDV')
X_standard_scaled = standard_scaler.fit_transform(data_df[features])
X_minmax_scaled = minmax_scaler.fit_transform(data_df[features])
X_robust_scaled = robust_scaler.fit_transform(data_df[features])

#Función para dividir datos, entrenar y evaluar el modelo
def train_evaluate(X, y):
    # Divido los datos en conjuntos de entrenamiento y prueba
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)

    # Creo y entrenar el modelo de regresión lineal
    model = LinearRegression()
    model.fit(X_train, y_train)

    # Realizo predicciones sobre el conjunto de prueba
    y_pred = model.predict(X_test)

    # Calculo y mostrar métricas
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    print(f"MSE: {mse}, R^2: {r2}")

#Entreno y evaluo el modelo para cada conjunto de características escaladas
print("Resultados usando StandardScaler:")
train_evaluate(X_standard_scaled, y)

print("\nResultados usando MinMaxScaler:")
train_evaluate(X_minmax_scaled, y)

print("\nResultados usando RobustScaler:")
train_evaluate(X_robust_scaled, y)

"""### **3. Con las características proyectadas sobre los dos primeros componentes principales. Puedes utilizar cualquier implementación de PCA en Python y realizar la proyección del dataset sólo sobre los primeros dos componentes principales.**"""

from sklearn.decomposition import PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_standard_scaled)
print("Resultados usando PCA con 2 componentes principales:")
train_evaluate(X_pca, y)

#Conclusión: al reducir el dataset a solo dos componentes principales, se pierde información crítica necesaria para predecir con precisión el valor de las viviendas.

"""### **4. Usando binning y características de interacción. Ten en cuenta que binning aplica solamente para características continuas**

"""

# RM (número medio de habitaciones por vivienda)
# Ejemplo de binning para la característica 'RM'
bins = [0, 5, 6, 7, 8, np.inf]  # Define los límites de los bins
labels = ['1-5', '6', '7', '8', '9+']  # Etiquetas para los bins
data_df['RM_binned'] = pd.cut(data_df['RM'], bins=bins, labels=labels, right=False)

#Creo las características de interacción

from sklearn.preprocessing import PolynomialFeatures

# Asumiendo que quieres crear una interacción entre 'RM_binned' (como variable dummy) y 'LSTAT'
data_df = pd.get_dummies(data_df, columns=['RM_binned'])  # Convierte 'RM_binned' en variables dummy
# Selecciona 'LSTAT' y las dummies de 'RM_binned' para las características de interacción
features_interaction = ['LSTAT'] + [col for col in data_df.columns if 'RM_binned' in col]
# Utiliza PolynomialFeatures para crear las características de interacción
interaction = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)
X_interaction = interaction.fit_transform(data_df[features_interaction])
# Ahora X_interaction contiene las características originales y sus interacciones

#Entreno y evaluo un modelo con las nuevas características
# Ejemplo de división de datos, entrenamiento y evaluación (usando LinearRegression como ejemplo)
X_train, X_test, y_train, y_test = train_test_split(X_interaction, y, test_size=0.3, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"MSE: {mse}, R^2: {r2}")

#Conclusión: Aunque el modelo puede explicar aproximadamente el 58% de la variabilidad en los precios de las viviendas, todavía hay un margen significativo de error en las predicciones

"""### **5. Usando características polinómicas. El grado que escojas debe ser sustentado con un análisis de múltiples opciones entre 2 y 12. Escoge el grado que proporcione un mejor desempeño.**"""

#me tocó ajustar entre 2 y 6 por el consumo de memoria

from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Análisis para encontrar el grado óptimo de características polinómicas
X = data_df.drop('MEDV', axis=1)  # Asumiendo que ya está estandarizado si es necesario
y = data_df['MEDV']

# Divido los datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Se inicializan listas para almacenar métricas
mse_scores = []
r2_scores = []

# Analizo grados polinómicos de 2 a 12
for degree in range(2, 7):
    poly = PolynomialFeatures(degree=degree)
    X_train_poly = poly.fit_transform(X_train)
    X_test_poly = poly.transform(X_test)

    model = LinearRegression()
    model.fit(X_train_poly, y_train)

    y_pred = model.predict(X_test_poly)

    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    mse_scores.append(mse)
    r2_scores.append(r2)

    print(f"Grado {degree}: MSE = {mse}, R^2 = {r2}")

#Escojo el grado óptimo
# Identificar el grado con el mejor R^2
optimal_degree_r2 = np.argmax(r2_scores) + 2  # +2 porque el rango comienza en 2

# Identifico el grado con el menor MSE
optimal_degree_mse = np.argmin(mse_scores) + 2  # +2 por la misma razón

print(f"Grado óptimo según R^2: {optimal_degree_r2}")
print(f"Grado óptimo según MSE: {optimal_degree_mse}")

#Conclusión: Los resultados muestran que el modelo con características polinómicas de grado 2 ofrece el mejor desempeño, a medida que aumentamos el grado de las características polinómicas más allá de 2, el rendimiento del modelo se deteriora significativamente

"""## **¿Qué estrategia de ingeniería de características ofrece los mejores resultados y por qué?**

Características Polinómicas de Grado 2 ofrecen los mejores resultados en comparación con las otras estrategias de ingeniería de características examinadas, incluyendo binning, características de interacción, y características polinómicas de grados superiores a 2.

# Fin del programa
"""